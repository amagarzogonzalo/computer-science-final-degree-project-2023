{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8TcVxKw5w-N"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam as AdamLegacy\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from scipy.spatial.distance import cosine\n",
        "import seaborn as sns\n",
        "from keras.regularizers import l1, l2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZ3-gs9pCqo_"
      },
      "outputs": [],
      "source": [
        "def add_average_ratings(movies, ratings):\n",
        "    \"\"\"\n",
        "    Calcula la valoración media para cada película y añade esta información a la tabla de películas.\n",
        "    \"\"\"\n",
        "    average_ratings = ratings.groupby('MovieID')['Rating'].mean()\n",
        "    movies = movies.merge(average_ratings, on='MovieID', how='left')\n",
        "    movies = movies.rename(columns={\"Rating\": \"AverageRating\"})\n",
        "    movies['AverageRating'].fillna(2.5, inplace=True)\n",
        "    return movies \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTiPrxaw1Osf"
      },
      "outputs": [],
      "source": [
        "def load_movielens_1m_from_files():\n",
        "    \"\"\"\n",
        "    Cargar los datos en 3 tablas distintas.\n",
        "    \"\"\"\n",
        "    movies = pd.read_csv(\n",
        "        \"movies.dat\",\n",
        "        sep=\"::\",\n",
        "        header=None,\n",
        "        names=[\"MovieID\", \"Title\", \"Genres\"],\n",
        "        engine=\"python\",\n",
        "        encoding=\"latin-1\",\n",
        "    )\n",
        "\n",
        "    ratings = pd.read_csv(\n",
        "        \"ratings.dat\",\n",
        "        sep=\"::\",\n",
        "        header=None,\n",
        "        names=[\"UserID\", \"MovieID\", \"Rating\", \"Timestamp\"],\n",
        "        engine=\"python\",\n",
        "        encoding=\"latin-1\",\n",
        "    )\n",
        "\n",
        "    users = pd.read_csv(\n",
        "        \"users.dat\",\n",
        "        sep=\"::\",\n",
        "        header=None,\n",
        "        names=[\"UserID\", \"Gender\", \"Age\", \"Occupation\", \"Zip-code\"],\n",
        "        engine=\"python\",\n",
        "        encoding=\"latin-1\",\n",
        "    )\n",
        "    movies = add_average_ratings(movies, ratings)\n",
        "    return movies, ratings, users\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJ7h93Sgy95U"
      },
      "outputs": [],
      "source": [
        "def plot_expert_weights(weights_history):\n",
        "  \"\"\"\n",
        "  Mostrar los pesos de cada experto al final de la simulación.\n",
        "  \"\"\"\n",
        "  plt.plot(weights_history[:, 0], label='Epsilon-Greedy (e=0.95)', color='blue')\n",
        "  plt.plot(weights_history[:, 1], label='Red Neuronal', color='orange')\n",
        "  plt.plot(weights_history[:, 2], label='Filtro Colaborativo basado en usuarios', color='green')  \n",
        "  plt.plot(weights_history[:, 3], label='Mejor brazo', color='red')  \n",
        "\n",
        "\n",
        "  plt.legend()\n",
        "  plt.xlabel('Ronda')\n",
        "  plt.ylabel('Pesos de los expertos')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZGmIKhdzZgB"
      },
      "outputs": [],
      "source": [
        "def plot_rewards(real_rewards, fake_rewards):\n",
        "  \"\"\"\n",
        "  Comprobamos a ver los resultados de los tipos de recompensas.\n",
        "  \"\"\"\n",
        "  plt.figure(figsize=(10,5), dpi=100)\n",
        "  plt.plot(real_rewards, label='Real Rewards', color='blue')\n",
        "  plt.legend()\n",
        "  plt.xlabel('Ronda')\n",
        "  plt.ylabel('Real Rewards')\n",
        "  plt.show()\n",
        "\n",
        "  plt.figure(figsize=(10,5), dpi=100)\n",
        "  plt.plot(fake_rewards, label='Fake Rewards', color='red')\n",
        "  plt.legend()\n",
        "  plt.xlabel('Ronda')\n",
        "  plt.ylabel('Fake Rewards')\n",
        "  plt.show()\n",
        "  plot_average_rewards(real_rewards, fake_rewards)\n",
        "\n",
        "\n",
        "def plot_average_rewards(real_rewards, fake_rewards):\n",
        "  \"\"\"\n",
        "  Comprobamos a ver los resultados de las recompensas medias.\n",
        "  \"\"\"\n",
        "  real_avg_rewards = np.cumsum(real_rewards) / np.arange(1, len(real_rewards) + 1)\n",
        "\n",
        "  plt.figure(figsize=(10,5), dpi=100)\n",
        "  plt.plot(real_avg_rewards, label='Recompensa media', color='blue')\n",
        "  plt.legend()\n",
        "  plt.xlabel('Ronda')\n",
        "  plt.ylabel('Recompensas medias')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_reward_histogram(realrewards, fakerewards):\n",
        "  \"\"\" \n",
        "  Histograma con las recompensas obtenidas.\n",
        "  \"\"\"\n",
        "  plt.figure()\n",
        "  plt.hist(realrewards, bins=10, alpha=0.5, label=\"Recompensas\")\n",
        "  plt.xlabel('Recompensa')\n",
        "  plt.ylabel('Frecuencia')\n",
        "  plt.title('Histograma de recompensas')\n",
        "  plt.legend(loc='upper right')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_violin_rewards_by_category(reward_data):\n",
        "    plt.figure(figsize=(18, 10))\n",
        "    sns.violinplot(x='Category', y='Reward', data=reward_data)\n",
        "    plt.title('Gráfico de violín de recompensas por brazo')\n",
        "    plt.xlabel('Categoría')\n",
        "    plt.ylabel('Recompensa')\n",
        "    plt.show()\n",
        "\n",
        "def plot_scatterplot_age_vs_rewards(reward_data):\n",
        "    plt.figure(figsize=(18, 10))\n",
        "    reward_data['Age'] = reward_data['Age'].astype(int)\n",
        "    sns.scatterplot(x='Age', y='Reward', data=reward_data)\n",
        "    plt.title('Scatterplot de Edad vs Recompensa')\n",
        "    plt.xlabel('Edad')\n",
        "    plt.ylabel('Recompensa')\n",
        "    x_ticks = sorted(reward_data['Age'].unique())\n",
        "    plt.xticks(ticks=x_ticks, labels=x_ticks)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def plot_stacked_area(reward_by_context_category):\n",
        "    reward_by_context_category.plot(kind='area', stacked=True, figsize=(10, 7))\n",
        "    plt.title('Recomendaciones de películas a lo largo del tiempo')\n",
        "    plt.ylabel('Número de recomendaciones')\n",
        "    plt.xlabel('Tiempo')\n",
        "    plt.show()\n",
        "\n",
        "def plot_contour_chart(reward_by_context_category):\n",
        "    sns.kdeplot(data=reward_by_context_category, x=\"Age\", y=\"Reward\", fill=True)\n",
        "    plt.title('Gráfico de contorno de Edad vs Recompensa')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_boxplot_rewards_by_gender(reward_by_context_category):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.boxplot(x='Gender', y='Reward', data=reward_by_context_category)\n",
        "    plt.title('Boxplot de Recompensas por Género')\n",
        "    plt.show()\n",
        "\n",
        "def plot_barplot_avg_rewards_by_age(reward_by_context_category):\n",
        "    avg_rewards = reward_by_context_category.groupby('Age')['Reward'].mean().reset_index()\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(x='Edad', y='Recompensa', data=avg_rewards)\n",
        "    plt.title('Barplot de Recompensas Medias por Grupo de Edad')\n",
        "    plt.show()\n",
        "\n",
        "def plot_densityplot_rewards_by_gender(reward_by_context_category):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    for gender in reward_by_context_category['Gender'].unique():\n",
        "        sns.kdeplot(reward_by_context_category[reward_by_context_category['Gender']==gender]['Reward'], label=gender)\n",
        "    plt.title('Densityplot de Recompensas por Género')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "vH54vm8UyOvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FOGNZ02W_23"
      },
      "outputs": [],
      "source": [
        "def plot_cumulative_rewards(cumulative_rewards):\n",
        "  \"\"\"\n",
        "  Gráfico auxiliar para pintar las recompensas acumuladas.\n",
        "  \"\"\"\n",
        "  plt.figure()\n",
        "  plt.plot(cumulative_rewards)\n",
        "  plt.xlabel(\"Ronda\")\n",
        "  plt.ylabel(\"Recompensa acumulativa\")\n",
        "  plt.title(\"Evolución de la recompensa acumulativa a lo largo del tiempo\")\n",
        "\n",
        "\n",
        "def plot_heatmap(reward_by_context_category):\n",
        "    \"\"\"\n",
        "    Mapa de calor de las recompensas-contextos.\n",
        "    \"\"\"\n",
        "    age_ranges = reward_by_context_category['Age'].unique()\n",
        "\n",
        "    for age_range in age_ranges:\n",
        "        heatmap_data_age = reward_by_context_category[reward_by_context_category['Age'] == age_range]\n",
        "        heatmap_data = heatmap_data_age.pivot_table(values='Reward', index=['Gender', 'Occupation'], columns='Category')\n",
        "\n",
        "        plt.figure(figsize=(18, 10))\n",
        "        sns.heatmap(heatmap_data, annot=True, cmap='coolwarm', linewidths=.5, fmt=\".2f\", cbar_kws={'label': 'Recompensa media'})\n",
        "        plt.title(f'Mapa de calor de recompensas por categorías y contexto (Edad: {age_range})')\n",
        "        plt.xlabel('Categoría')\n",
        "        plt.ylabel('Contexto (Género, Ocupación)')\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7F0PzxMmVq7s"
      },
      "outputs": [],
      "source": [
        "def create_user_subset(users, num_userstrain, num_usersexp4):\n",
        "    \"\"\"\n",
        "    Crea dos subconjuntos de usuarios a partir de la tabla de usuarios.\n",
        "    \"\"\"\n",
        "    shuffled_users = users.sample(frac=1).reset_index(drop=True)  \n",
        "    train_users = shuffled_users.iloc[:num_userstrain].reset_index(drop=True)  \n",
        "    exp4_users = shuffled_users.iloc[num_userstrain:num_userstrain + num_usersexp4].reset_index(drop=True)  \n",
        "\n",
        "    return train_users, exp4_users\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zU11b5BW1_E"
      },
      "outputs": [],
      "source": [
        "class Hedge:\n",
        "    def __init__(self, num_experts, eta):\n",
        "        self.num_experts = num_experts\n",
        "        self.eta = eta\n",
        "        self.weights = np.ones(num_experts)\n",
        "        self.probabilities = self.weights / np.sum(self.weights)\n",
        "\n",
        "    def get_probabilities(self):\n",
        "        return self.probabilities\n",
        "\n",
        "    def update_weights(self, costs):\n",
        "        costs = np.where(np.isnan(costs), 0, costs)\n",
        "\n",
        "        for e in range(self.num_experts):\n",
        "            self.weights[e] *= (1 - self.eta) ** costs[e]\n",
        "        \n",
        "        min_weight = np.min(self.weights)\n",
        "        if min_weight < 1e-50:\n",
        "            self.weights = np.where(self.weights == 0, 1e-20, self.weights)\n",
        "            self.weights = self.weights / 1e-20\n",
        "\n",
        "        self.weights = np.where(np.isnan(self.weights), 0.0001, self.weights) \n",
        "       \n",
        "        self.probabilities = self.weights / (np.sum(self.weights)+ 1e-10)\n",
        "\n",
        "        num_nan_values = np.count_nonzero(np.isnan(self.probabilities))\n",
        "        if num_nan_values > 0:\n",
        "          non_nan_prob_sum = np.sum(self.probabilities[~np.isnan(self.probabilities)])\n",
        "          correction = (1 - non_nan_prob_sum) / num_nan_values\n",
        "          self.probabilities = np.where(np.isnan(self.probabilities), 0.0001, self.probabilities + correction)\n",
        "        self.probabilities = self.probabilities / (np.sum(self.probabilities))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHWzYUzAPt3c"
      },
      "outputs": [],
      "source": [
        "def add_random_reward(movie_avg_rating):\n",
        "    \"\"\"\n",
        "    Añadir aleatoriedad a la recompensa.\n",
        "    \"\"\"\n",
        "    reward = movie_avg_rating + np.random.normal(-0.5, 0.5)\n",
        "    reward = reward.clip(1, 5)\n",
        "    return reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0WGRk0sKuiQ"
      },
      "outputs": [],
      "source": [
        "def preprocess_user_data(user):\n",
        "  \"\"\"\n",
        "  Preprocesar el contexto.\n",
        "  \"\"\"\n",
        "  user[\"Gender\"] = 1 if user[\"Gender\"] == 'M' else 0\n",
        "\n",
        "  user[\"Age\"] = user[\"Age\"] \n",
        "  user[\"Occupation\"] = user[\"Occupation\"] \n",
        "\n",
        "  context = user[[\"Gender\", \"Age\", \"Occupation\"]].values.astype(np.float32)\n",
        "  return context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SpfCfu2vt3Gr"
      },
      "outputs": [],
      "source": [
        "def get_reward(user, chosen_category, movies, ratings, user_movies_dict, bbdd, user_moviesrewards_dict):\n",
        "  \"\"\"\n",
        "  Calcula la recompensa o de la bbdd rating o aleatoria. Falta meter el sesgo.\n",
        "  \"\"\"\n",
        "  user_movies = ratings[ratings[\"UserID\"] == user[\"UserID\"]]\n",
        "  user_category_movies = user_movies[user_movies[\"MovieID\"].isin(movies[movies[\"Genres\"].str.contains('|'.join(chosen_category))][\"MovieID\"])]\n",
        "  reward = -1\n",
        "  categories = []\n",
        "  chosen_movie = None\n",
        "  if not user_category_movies.empty:\n",
        "      ind = 0\n",
        "      while ind < len(user_category_movies):\n",
        "          chosen_movie = user_category_movies.iloc[ind] \n",
        "          if chosen_movie[\"MovieID\"].item() not in user_movies_dict[user[\"UserID\"]]:\n",
        "              reward = chosen_movie[\"Rating\"] / 5\n",
        "              bbdd = True\n",
        "              movie_id = chosen_movie[\"MovieID\"]\n",
        "              movie_row = movies.loc[movies['MovieID'] == movie_id]\n",
        "              categories = movie_row['Genres'].values[0].split('|')\n",
        "              break\n",
        "          ind += 1\n",
        "  if reward == -1 or user_category_movies.empty or chosen_movie is None: \n",
        "    category_movies = movies[movies[\"Genres\"] == (chosen_category)]\n",
        "    user_unwatched_movies = category_movies[~category_movies[\"MovieID\"].isin(user_movies_dict[user[\"UserID\"]])]\n",
        "    if not user_unwatched_movies.empty:\n",
        "        chosen_movie = user_unwatched_movies.sample().iloc[0]\n",
        "        reward = add_random_reward(chosen_movie[\"AverageRating\"]) / 5\n",
        "  \n",
        "  category_movies = movies[movies[\"Genres\"] == (chosen_category)]\n",
        "  user_unwatched_movies = category_movies[~category_movies[\"MovieID\"].isin(user_movies_dict[user[\"UserID\"]])]\n",
        "    \n",
        "  \n",
        "\n",
        "  if chosen_movie is not None and reward != -1:\n",
        "    user_movies_dict[user[\"UserID\"]] = np.append(user_movies_dict[user[\"UserID\"]], chosen_movie[\"MovieID\"])\n",
        "    user_moviesrewards_dict[user[\"UserID\"]].append({\"MovieID\": chosen_movie[\"MovieID\"], \"Reward\": reward})\n",
        "    \n",
        "\n",
        "\n",
        "  return reward, user_movies_dict, bbdd, categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaCoW-MRXOSP"
      },
      "outputs": [],
      "source": [
        "class Exp4:\n",
        "    def __init__(self, experts, eta, gamma, num_arms):\n",
        "        self.experts = experts\n",
        "        self.num_experts = len(experts)\n",
        "        self.chosenArms = np.zeros(len(experts)) \n",
        "        self.num_arms = num_arms\n",
        "        self.hedge = Hedge(self.num_experts, eta)\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def sampleExpert(self): \n",
        "        et = np.random.choice(self.num_experts, p=self.hedge.get_probabilities())\n",
        "        \n",
        "        if np.random.random() < (1 - self.gamma):\n",
        "            at = self.chosenArms[et] \n",
        "        else:\n",
        "            at = np.random.randint(self.num_arms)\n",
        "        \n",
        "        return at\n",
        "\n",
        "    def sampleArmExperts(self, context):\n",
        "        for e in range (self.num_experts):\n",
        "            self.chosenArms[e] = self.experts[e].select_arm(context)\n",
        "\n",
        "    def update(self, reward, at, context):\n",
        "        cost = 1 - reward \n",
        "        fake_costs = np.zeros(self.num_experts) \n",
        "        pt = 0  \n",
        "        for e in range(self.num_experts):\n",
        "            pt += self.experts[e].get_arm_probability(at, context) * self.hedge.get_probabilities()[e]\n",
        "\n",
        "        for e in range(self.num_experts):\n",
        "            if at == self.chosenArms[e]:\n",
        "              if np.isnan(pt):\n",
        "                pt = 0.1\n",
        "              fake_costs[e] = cost / (pt+1e-10)               \n",
        "        self.hedge.update_weights(fake_costs)\n",
        "\n",
        "    def get_weights(self):\n",
        "        return self.hedge.get_probabilities()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OiihdsywfaU7"
      },
      "outputs": [],
      "source": [
        "class EpsilonGreedy:\n",
        "    def __init__(self, epsilon, num_arms):\n",
        "        self.epsilon = epsilon\n",
        "        self.num_arms = num_arms\n",
        "        self.counts = np.zeros(num_arms)\n",
        "        self.values = np.zeros(num_arms)\n",
        "        \n",
        "    def select_arm(self, context):\n",
        "        if np.random.random() > self.epsilon:\n",
        "            return np.argmax(self.values)\n",
        "        else:\n",
        "            return np.random.randint(self.num_arms)\n",
        "        \n",
        "    def update(self, context, arm, reward):\n",
        "        self.counts[arm] += 1\n",
        "        n = self.counts[arm]\n",
        "        value = self.values[arm]\n",
        "        new_value = ((n - 1) / n) * value + (1 / n) * reward\n",
        "        self.values[arm] = new_value\n",
        "        \n",
        "    def get_arm_probabilities(self, context):\n",
        "        return self.values\n",
        "\n",
        "    def get_arm_probability(self, arm, context):\n",
        "      total_counts = np.sum(self.counts)\n",
        "      if self.epsilon == 0: \n",
        "        if arm == np.argmax(self.values):\n",
        "          return 1\n",
        "        else:\n",
        "          return 0\n",
        "      else:\n",
        "        if total_counts == 0:\n",
        "          return np.ones(self.num_arms) / self.num_arms\n",
        "        if arm == np.argmax(self.values):\n",
        "          explote = 1\n",
        "        else:\n",
        "          explote = 0\n",
        "        explore = (self.epsilon/self.num_arms)\n",
        "        return explote*(1-self.epsilon)+explore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-ARoUjEzdxy"
      },
      "outputs": [],
      "source": [
        "class NeuralNetworkExpert:\n",
        "    def __init__(self, num_arms, input_shape=(3, ), learning_rate=0.05):\n",
        "        self.num_arms = num_arms\n",
        "        self.model = self.build_model(input_shape, learning_rate)\n",
        "\n",
        "    def build_model(self, input_shape, learning_rate):\n",
        "        model = Sequential()\n",
        "        model.add(Dense(120, activation='relu', input_shape=input_shape, kernel_regularizer=l2(0.015)))\n",
        "        model.add(Dense(70, activation='relu', kernel_regularizer=l2(0.015)))\n",
        "        model.add(Dense(self.num_arms, activation='softmax'))\n",
        "        model.compile(loss='categorical_crossentropy', optimizer=AdamLegacy(learning_rate=learning_rate))\n",
        "        return model\n",
        "    def get_arm_probability(self, arm, context):\n",
        "        probabilities = self.get_arm_probabilitiess(context)\n",
        "        if np.argmax(probabilities) == arm:\n",
        "          return 1\n",
        "        else:\n",
        "          return 0 \n",
        "\n",
        "    def get_arm_probabilitiess(self, context):\n",
        "        context = context.reshape(1, -1)\n",
        "        return self.model.predict(context, verbose=0)[0]\n",
        "        \n",
        "\n",
        "    def select_arm(self, context):\n",
        "        probabilities = self.get_arm_probabilitiess(context)\n",
        "        \n",
        "        return np.argmax(probabilities)\n",
        "\n",
        "    def update(self, X_history, chosen_arm_history, reward_history, batch_size=32, epochs=170):\n",
        "        X_train = []\n",
        "        y_train = []\n",
        "        \n",
        "        for i in range(len(X_history)):\n",
        "            X = X_history[i]\n",
        "            chosen_arms = chosen_arm_history[i]\n",
        "            reward = reward_history[i]\n",
        "            \n",
        "            for chosen_arm in chosen_arms: \n",
        "                X_train.append(X)\n",
        "                y_weighted = np.zeros(self.num_arms)\n",
        "                y_weighted[chosen_arm] = reward\n",
        "                y_weighted /= np.sum(y_weighted)\n",
        "                y_train.append(y_weighted)\n",
        "                \n",
        "        X_train = np.array(X_train)\n",
        "        y_train = np.array(y_train)\n",
        "        \n",
        "        self.model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=0)\n",
        "         \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4pQbzUb5EETa"
      },
      "outputs": [],
      "source": [
        "def similarity(u, v):\n",
        "    return 1 - cosine(u, v)\n",
        "\n",
        "class NeighborhoodBasedCollaborativeFiltering:\n",
        "    def __init__(self, n_arms, k_nearest=5):\n",
        "        self.n_arms = n_arms\n",
        "        self.k_nearest = k_nearest\n",
        "        self.contexts = []\n",
        "        self.chosen_arms = []\n",
        "        self.rewards = []\n",
        "\n",
        "    def select_arm(self, context):\n",
        "      if not self.contexts:\n",
        "          return np.random.choice(self.n_arms)\n",
        "\n",
        "      similarities = [similarity(context, c) for c in self.contexts]\n",
        "      nearest_indices = np.argsort(similarities)[-self.k_nearest:]\n",
        "\n",
        "      arm_rewards = np.zeros(self.n_arms)\n",
        "      arm_counts = np.zeros(self.n_arms)\n",
        "\n",
        "      for index in nearest_indices:\n",
        "          chosen_arms_list = self.chosen_arms[index]\n",
        "          reward = self.rewards[index]\n",
        "          \n",
        "          if chosen_arms_list and not isinstance(chosen_arms_list[0], list):\n",
        "              chosen_arms_list = [chosen_arms_list]\n",
        "          for chosen_arms in chosen_arms_list:\n",
        "            for arm in chosen_arms:\n",
        "              arm_rewards[arm] += reward\n",
        "              arm_counts[arm] += 1\n",
        "\n",
        "      arm_counts[arm_counts == 0] = 1\n",
        "      arm_values = arm_rewards / arm_counts\n",
        "      return np.argmax(arm_values)\n",
        "\n",
        "    \n",
        "    \n",
        "  \n",
        "    \n",
        "    def update(self, X_history, chosen_arm_history, reward_history):\n",
        "        self.contexts.extend(X_history)\n",
        "        self.chosen_arms.extend(chosen_arm_history)\n",
        "        self.rewards.extend(reward_history)\n",
        "      \n",
        "    def get_arm_probability(self, arm, context):\n",
        "      if not self.contexts:\n",
        "          return 1 / self.n_arms\n",
        "\n",
        "      similarities = [similarity(context, c) for c in self.contexts]\n",
        "      nearest_indices = np.argsort(similarities)[-self.k_nearest:]\n",
        "\n",
        "      arm_rewards = np.zeros(self.n_arms)\n",
        "      arm_counts = np.zeros(self.n_arms)\n",
        "\n",
        "      for index in nearest_indices:\n",
        "          chosen_arms_list = self.chosen_arms[index]\n",
        "          reward = self.rewards[index]\n",
        "\n",
        "          if chosen_arms_list and not isinstance(chosen_arms_list[0], list):\n",
        "              chosen_arms_list = [chosen_arms_list]\n",
        "          \n",
        "          for chosen_arm in chosen_arms_list:\n",
        "              if arm in chosen_arm:\n",
        "                  arm_rewards[arm] += reward\n",
        "                  arm_counts[arm] += 1\n",
        "\n",
        "      arm_counts[arm_counts == 0] = 1\n",
        "      arm_values = arm_rewards / arm_counts\n",
        "      arm_values[np.isnan(arm_values)] = 0 \n",
        "      total_value = np.sum(arm_values)\n",
        "      if np.argmax(arm_values[arm]):\n",
        "        return 1\n",
        "      else:\n",
        "         return 0\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWfnXDGdKk-Z"
      },
      "outputs": [],
      "source": [
        "def createExperts(num_arms):\n",
        "  \"\"\"\n",
        "  Se crea la lista de expertos.\n",
        "  \"\"\"\n",
        "  return [EpsilonGreedy(0.95, num_arms)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_P2B1yI0ZmCA"
      },
      "outputs": [],
      "source": [
        "def init_simulation(movies, users, perc_train):\n",
        "  \"\"\"\n",
        "  Se preparan ciertas variables de la simulacion\n",
        "  \"\"\"\n",
        "  movie_genres = movies[\"Genres\"].apply(lambda x: x.split(\"|\")) \n",
        "  categories = np.unique(np.concatenate(movie_genres)) \n",
        "  genre_dict = {category: num for num, category in enumerate(categories)}\n",
        "  total_users = len(users)\n",
        "  num_userstrain = int(total_users * perc_train)\n",
        "  num_usersexp4 = total_users - num_userstrain\n",
        "\n",
        "  usersubset1, usersubset2 = create_user_subset(users, num_userstrain, num_usersexp4)\n",
        "  \n",
        "  num_arms = len(categories)\n",
        "  experts = createExperts(num_arms) \n",
        "\n",
        "  user_movies_dict = np.empty(len(users), dtype=object)\n",
        "  user_movies_dict[:] = [[] for _ in range(len(users))]\n",
        "  user_movies_dict = dict(zip(users[\"UserID\"], user_movies_dict))\n",
        "  return usersubset1, usersubset2, num_arms, categories, user_movies_dict, experts, genre_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MguVpr_bGx6Y"
      },
      "outputs": [],
      "source": [
        "def first_part_simulation(user_movies_dict, num_arms, categories, user_subset, genre_dict, roundstraining):\n",
        "  \"\"\"\n",
        "  Entrenamiento de políticas.\n",
        "  \"\"\"\n",
        "  experts = createExperts(num_arms)\n",
        "  user_moviesrewards_dict = {user: [] for user in users[\"UserID\"]}\n",
        "\n",
        "  context_history = []\n",
        "  chosen_arm_history = []\n",
        "  chosen_arms_history = []\n",
        "  reward_history = []\n",
        "  j = 0\n",
        "  i = 0\n",
        "  \n",
        "  greedy = EpsilonGreedy(0, num_arms)\n",
        "  num_rounds = roundstraining * len(user_subset)\n",
        "  while i < num_rounds:\n",
        "      if j == (len(user_subset)-1):\n",
        "        j = 0\n",
        "      user = user_subset.iloc[j % num_rounds]\n",
        "      context = preprocess_user_data(user[[\"Gender\", \"Age\", \"Occupation\"]])\n",
        "            \n",
        "      for e in experts:\n",
        "        chosen_arm = e.select_arm(context)\n",
        "        chosen_category = categories[chosen_arm]\n",
        "        bbdd = False\n",
        "        reward, user_movies_dict, bbdd, categoriesel = get_reward(user, chosen_category, movies, ratings, user_movies_dict, bbdd, user_moviesrewards_dict)\n",
        "\n",
        "        genres_nums = [genre_dict[genre] for genre in categoriesel]\n",
        "        e.update(context, int(chosen_arm), reward)\n",
        "        if bbdd == True: \n",
        "          for genre in genres_nums:\n",
        "            greedy.update(context, int(genre), reward)\n",
        "          chosen_arms_history.append(genres_nums)\n",
        "          context_history.append(context)\n",
        "          chosen_arm_history.append(genres_nums)\n",
        "          reward_history.append(reward)\n",
        "        j +=1\n",
        "        i += 1\n",
        " \n",
        " \n",
        "  neigh = NeighborhoodBasedCollaborativeFiltering(num_arms)\n",
        "  neigh.update(context_history, chosen_arm_history, reward_history)\n",
        "  nn = NeuralNetworkExpert (num_arms)\n",
        "  nn.update(context_history, chosen_arm_history, reward_history)\n",
        "  experts.append(nn) \n",
        "  experts.append(neigh)\n",
        "  experts.append(greedy)\n",
        "  return experts, user_movies_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtRSGlTra8bB"
      },
      "outputs": [],
      "source": [
        "def second_part_simulation(user_movies_dict, exp4, user_subset, categories, genre_dict):\n",
        "  \"\"\"\n",
        "  Esta es la segunda parte de la simulación, en la que los expertos ya han sido entrenados y el objetivo \n",
        "  es probar como exp4 sabe discernir entre la opinión de expertos.\n",
        "  \"\"\" \n",
        "  user_moviesrewards_dict = {user: [] for user in users[\"UserID\"]}\n",
        "\n",
        "  weights_history = []\n",
        "  realrewards = []\n",
        "  fakerewards = []\n",
        "  cumulative_rewards = [0] \n",
        "  reward_data = pd.DataFrame(columns=['Gender', 'Age', 'Occupation', 'Category', 'Reward'])\n",
        "  reward_dataFaketoo = pd.DataFrame(columns=['Gender', 'Age', 'Occupation', 'Category', 'Reward'])\n",
        "\n",
        "  j = 0\n",
        "  i = 0\n",
        "  num_rounds = len(user_subset) * 4\n",
        "  while i < num_rounds:\n",
        "      if j == (len(user_subset)-1):\n",
        "        j = 0\n",
        "      user = user_subset.iloc[j % num_rounds]\n",
        "      context = preprocess_user_data(user[[\"Gender\", \"Age\", \"Occupation\"]])\n",
        "\n",
        "      exp4.sampleArmExperts(context)\n",
        "      chosen_arm = int(exp4.sampleExpert())\n",
        "      chosen_category = categories[chosen_arm]\n",
        "      \n",
        "      bbdd = False\n",
        "      j += 1\n",
        "      i += 1\n",
        "      reward, user_movies_dict, bbdd,categoriesel = get_reward(user, chosen_category, movies, ratings, user_movies_dict, bbdd, user_moviesrewards_dict)\n",
        "      genres_nums = [genre_dict[genre] for genre in categoriesel]\n",
        "          \n",
        "\n",
        "      if bbdd == True:\n",
        "        exp4.update(reward, chosen_arm, context)\n",
        "        weights_history.append(exp4.get_weights())\n",
        "        realrewards.append(reward)\n",
        "       \n",
        "        for genre in genres_nums:\n",
        "          reward_data = pd.concat([reward_data, pd.DataFrame({'Gender': [user['Gender']],\n",
        "                                                   'Age': [user['Age']],\n",
        "                                                   'Occupation': [user['Occupation']],\n",
        "                                                   'Category': [categories[genre]],\n",
        "                                                   'Reward': [reward]})], ignore_index=True)\n",
        "\n",
        "      else:\n",
        "        fakerewards.append(reward)\n",
        "        cumulative_rewards.append(cumulative_rewards[-1] + reward)\n",
        "        reward_dataFaketoo = pd.concat([reward_dataFaketoo, pd.DataFrame({'Gender': [user['Gender']],\n",
        "                                                                   'Age': [user['Age']],\n",
        "                                                                   'Occupation': [user['Occupation']],\n",
        "                                                                   'Category': [chosen_category],\n",
        "                                                                   'Reward': [reward]})], ignore_index=True)\n",
        "\n",
        "  return np.array(weights_history), np.array(realrewards), np.array(fakerewards), np.array(cumulative_rewards), reward_data, reward_dataFaketoo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuh-WS0SUAFX"
      },
      "outputs": [],
      "source": [
        "def run_simulation(movies, ratings, users, perc_train, roundstraining):\n",
        "    \"\"\"\n",
        "    Ejecuta la simulación del algoritmo UCB1 con un número específico de usuarios.\n",
        "    \"\"\"\n",
        "    user_subset1, user_subset2, num_arms, categories, user_movies_dict, experts, genre_dict = init_simulation(movies, users, perc_train)\n",
        "    experts, user_movies_dict = first_part_simulation(user_movies_dict, num_arms, categories, user_subset1, genre_dict, roundstraining)\n",
        "    eta = 0.05\n",
        "    gamma = 0 \n",
        "    exp4 = Exp4(experts, eta, gamma, num_arms)\n",
        "    weights_history, realrewards, fakerewards, cumulative_rewards, reward_data, reward_dataFaketoo = second_part_simulation(user_movies_dict, exp4, user_subset2, categories, genre_dict)\n",
        "    reward_by_context_category = reward_data.groupby(['Gender', 'Age', 'Occupation', 'Category']).mean().reset_index()\n",
        "    reward_by_context_categoryFaketoo = reward_data.groupby(['Gender', 'Age', 'Occupation', 'Category']).mean().reset_index()\n",
        "    plot_expert_weights(weights_history)\n",
        "    print(\"Recompensa media: \", np.mean(realrewards) )\n",
        "    plot_average_rewards(realrewards, fakerewards)\n",
        "    plot_reward_histogram(realrewards, fakerewards)\n",
        "    plot_heatmap(reward_by_context_category)\n",
        "    plot_scatterplot_age_vs_rewards(reward_by_context_category)\n",
        "    plot_violin_rewards_by_category(reward_by_context_category)\n",
        "    plot_stacked_area(reward_by_context_category)\n",
        "    plot_contour_chart(reward_by_context_category)\n",
        "\n",
        "    plot_boxplot_rewards_by_gender(reward_by_context_category)\n",
        "    plot_densityplot_rewards_by_gender(reward_by_context_category)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQ8ZEsSdHl45"
      },
      "outputs": [],
      "source": [
        "movies, ratings, users =  load_movielens_1m_from_files()\n",
        "size = 1\n",
        "perc_train = 0.8\n",
        "roundstraining= 4\n",
        "\n",
        "num_users = len(users)\n",
        "subset_size = int(num_users * size)\n",
        "subset_users = users.iloc[:subset_size].reset_index(drop=True)\n",
        "\n",
        "run_simulation(movies, ratings, subset_users, perc_train, roundstraining)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}